\section*{Probability Terminology}

\textbf{Random experiment}: action with \textit{uncertain} outcome.

\textbf{Sample space} $\Omega$:
\begin{equation*}
    \Omega = \set{ \text{all possible outcomes} }
\end{equation*}

\textbf{Event} or \textbf{Event Space} $E$:
\begin{equation*}
    E \subseteq \Omega
\end{equation*}

\textbf{Complement Event} $\AbsComplement{E}$:

Event $\AbsComplement{E}$ is the event that $E$ does \textit{not} occur.
\begin{equation*}
    \AbsComplement{E} = \set{ \omega \in \Omega \given \omega \not\in E }
\end{equation*}

\textbf{Intersection of Events} $E_1 \cap E_2$:

$E_1 \cap E_2$ is event that $E_1$ and $E_2$ occurring \textit{together}.
\begin{equation*}
    E_1 \cap E_2 = \set{ \omega \in \Omega \given \omega \in E_1 \land \omega \in E_2 }
\end{equation*}

\textbf{Union of Events} $E_1 \cup E_2$:

$E_1 \cup E_2$ is the event that $E_1$ \textit{or} $E_2$ occurring or \textit{both}.
\begin{equation*}
    E_1 \cup E_2 = \set{ \omega \in \Omega \given \omega \in E_1 \lor \omega \in E_2 }
\end{equation*}

\textbf{Difference} $E_1 \setminus E_2$:

$E_1 \setminus E_2$ is the event that $E_1$ \textit{occurs} but \textit{not} $E_2$.
\begin{equation*}
    E_1 \setminus E_2 = \set{ \omega \in \Omega \given \omega \in E_1 \land \omega \not\in E_2 }
\end{equation*}

\textbf{Useful Identities}:
\begin{align*}
    \AbsComplement{A \cap B} &= \AbsComplement{A} \cup \AbsComplement{B} \\
    \AbsComplement{A \cup B} &= \AbsComplement{A} \cap \AbsComplement{B} \\
    A \cap (B \cup C) &= (A \cap B) \cup (A \cap C) \\
    A \cup (B \cap C) &= (A \cup B) \cap (A \cup C)
\end{align*}

\textbf{Disjoint} or \textbf{Mutually Exclusive} $E_1 \xor E_2$:

Given events $E_1$ and $E_2$ then they are \textit{disjoint} or \textit{mutually exclusive} iff:
\begin{equation*}
    E_1 \cap E_2 = \varnothing \iff E_1 \xor E_2
\end{equation*}

That is $E_1$ and $E_2$ \textit{cannot both occur}.

\textbf{Probability of an Event} $\Prob{E}$: \textit{proportion} of times that event $E$ is expected to occur in a large sample size.

\section*{Probability Properties}

\begin{gather*}
    \begin{aligned}
        \Prob{E} &\ge 0 \\
        \Prob{E} &\le 1 \\
        \Prob{\Omega} &= 1 \\
        \Prob{\varnothing} &= 0
    \end{aligned} \\
    \Prob{E_1 \cup E_2} = \Prob{E_1} + \Prob{E_2} - \Prob{E_1 \cap E_2}
\end{gather*}

\begin{equation*}
    E_1 \xor E_2 \iff \Prob{E_1 \cap E_2} = \Prob{E_1} + \Prob{E_2}
\end{equation*}

\section*{Probability Definition}

A \textbf{probability measure} $\mathrm{P} \colon E \to \Real$ associates each \textit{event} $E$ with a probability $\Prob{E}$ such that the three properties are satisfied:

\begin{enumerate}
    \item \textbf{Non-negative}:
    \begin{equation*}
        \Forall E \colon \Prob{E} \ge 0
    \end{equation*}
    \item \textbf{Sums to} $1$:
    \begin{equation*}
        \Prob{\Omega} = 1
    \end{equation*}
    \item \textbf{Countable additivity}: given \textit{events} $E_1, \dots, E_n$ such that each event is \textit{independent} from all other event:
    \begin{equation*}
        \Forall i, j \in [1, n] \colon i \ne j \to E_i \cap E_j = \varnothing
    \end{equation*}
    Then
    \begin{equation*}
        \Prob{E_1 \cup \cdots \cup E_n} = \Prob{E_1} + \cdots + \Prob{E_n}
    \end{equation*}
\end{enumerate}

\section*{Combinatorical Probability}

If all outcomes for a random experiment are \textit{equally} likely, then the probability of event $E$ is
\begin{equation*}
    \Prob{E} = \frac{ \mathrm{n}(E) }{ \mathrm{n}(\Omega) }
\end{equation*}

\section*{Conditional Probability}

The \textbf{conditional probability} of event $E_1$ occurring \textit{given} $E_2$ occurred
\begin{equation*}
    \Prob{E_1 \given E_2} = \frac{ \Prob{E_1 \cap E_2} }{ \Prob{E_2} }
\end{equation*}

\begin{remark}
    \textbf{Prosecutor's fallacy}:
    \begin{equation*}
        \Prob{E_1 \given E_2} \ne \Prob{E_2 \given E_1}
    \end{equation*}
\end{remark}

\subsection*{Independence}

Since $E_1$ and $E_2$ are \textit{independent} iff $\Prob{E_1 \cap E_2} = \Prob{E_1} \Prob{E_2}$, then if $\Prob{E_1} > 0$ and $\Prob{E_2} > 0$
\begin{align*}
    \Prob{E_1 \given E_2}
    &= \frac{\Prob{E_1 \cap E_2}}{\Prob{E_2}}
    = \frac{\Prob{E_1} \Prob{E_2}}{\Prob{E_2}}
    = \Prob{E_1} \\
    \Prob{E_2 \given E_1}
    &= \Prob{E_2}
\end{align*}

\section*{Probability Rules}

\subsection*{Bayes Theorem}

To compute $\Prob{E_1 \given E_2}$ given $\Prob{E_2 \given E_1}$
\begin{equation*}
    \Prob{E_1 \given E_2}
    = \frac{\Prob{E_2 \given E_1} \Prob{E_1}}{\Prob{E_2}}
\end{equation*}

\subsection*{Law of Total Probability}

Given \textit{mutually exclusive} events $E_1, \dots, E_n$ where $E \subseteq E_1 \cup \cdots \cup E_n$, then if $E$ occurs exactly one of $\set{ E_1, \dots, E_n }$ must occur (even if $n = \infty$):
\begin{align*}
    \Prob{E}
    &= \sum \nolimits_{i = 1}^{n} \Prob{E \cap E_i} \\
    &= \sum \nolimits_{i = 1}^{n} \Prob{E \given E_i} \Prob{E_i}
\end{align*}

\subsection*{Intersection of Events}

\begin{itemize}
    \item 2 events:
    \begin{equation*}
        \Prob{E_1 \cap E_2} = \Prob{E_2 \given E_1} \Prob{E_1}
    \end{equation*}
    \item 3 events:
    \begin{align*}
        &\ProbS{(E_1 \cap E_2) \cap E_3} \\
        = &\Prob{E_3 \given E_1 \cap E_2} \Prob{E_1 \cap E_2} \\
        = &\Prob{E_3 \given E_1 \cap E_2} \Prob{E_2 \given E_1} \Prob{E_1}
    \end{align*}
\end{itemize}

\subsection*{Union of Events}

\begin{itemize}
    \item 2 events:
    \begin{equation*}
        \Prob{E_1 \cup E_2} = \Prob{E_1} + \Prob{E_2} - \Prob{E_1 \cap E_2}
    \end{equation*}
    \item 3 events:
    \begin{align*}
        &\Prob{E_1 \cup E_2 \cup E_3} \\
        &= \begin{aligned}[t]
        &\Prob{E_1} + \Prob{E_2} + \Prob{E_3} \\
        &- \Prob{E_1 \cap E_2} \\
        &- \Prob{E_1 \cap E_3} \\
        &- \Prob{E_2 \cap E_3} \\
        &+ \Prob{E_1 \cap E_2 \cap E_3}
        \end{aligned}
    \end{align*}
    \item 3 events (\textit{mutually exclusive}):
    \begin{equation*}
        \Prob{E_1 \cup E_2 \cup E_3} = \Prob{E_1} + \Prob{E_2} + \Prob{E_3}
    \end{equation*}
\end{itemize}
